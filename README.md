This repository contains benchmarks allowing comparison of various quantum circuit/quantum program optimizers, mainly for the convenience of folks working on Feynman.

# Building And Running

## Repository Layout

| Folder Or File | Description |
|--|--|
| `/bench.py` | The script that does all the real work |
| `/bench` | Data and output folders for `bench.py` |
| `/bench/build` | Output folder for benchmarks, can be removed to clean the repository |
| `/bench/resources` | Source `.qc` and `.qasm` files used as input by the benchmark tests |
| `/<subject>` | Source and build folders for test subjects i.e. `feynman`, `mlvoqc`, `quartz`, `queso`, `quizx`... |

## Basic Instructions

Before running any benchmarks, you should build the programs being benchmarked. Their needs are very diverse, and I elected not to automate this at the moment because without containerizing the whole build it's challenging to automate installing build dependencies without polluting the user's environment. The programs being benchmarked have diverse build dependencies -- Feynman needs Haskell (specifically some vaguely recent version of GHC), others need variously Java, C++, OCaml, or Rust... you should consult the documentation for those programs for details.

For some test subjects, as of this writing QUESO and Quartz, it's necessary to generate rule files. Those packages include the tools to do this, but we don't automate the process. The rules should be checked into the repository, but if you want to regenerate these files there should be instructions in the subject folder.

For `bench.py` itself you will need the `ninja` package, but otherwise it just needs a recent 3.x version of python (I am developing with 3.10, older might also work and newer of course barring breaking changes).

Once the test subjects are built, run `python3 bench.py <suite>` where `<suite>` is something like `popl25`. For a list of current available options you can run `python3 bench.py --list-suites`. Output will be generated in `bench/build/<suite>/<timestamp>/<subject>/...`.

## Platform Compatibility

This system is currently maintained targeting Ubuntu 22.04. It will probably work on other unixes and e.g. MacOS with Homebrew or similar, but probably not on Windows (and if that's your platform, consider running it under WSL). If you want to port it, the Ninja generation will most likely need to be updated somewhat significantly, but the rest might just work.

# Design

## Glossary

| Term | Meaning |
|--|--|
| Benchmark, benchmark suite, suite | A collection of tests, test subjects, and specific test procedures (including which data is collected from the tests) |
| Measurable | A data point of interest which is generated by a test, e.g. T-gate count, runtime, peak memory use |
| Subject, test subject | A program or framework being benchmarked, e.g. Feynman, Quartz, VOQC... |
| Resource | Some input to a test, for now just a `.qc` or `.qasm` file |
| Test | A single test run as part of a suite, generally a particular quantum circuit or program being transformed |
| Transformed source | The output from a test process -- for now the objective is optimization, but that goal isn't always achieved, and maybe in future it's not even the goal, so we just say transformed |

Note that "source" usually means `.qc` or `.qasm` source, i.e., the input _and output_ of the test subject. The nomenclature could then be "source" or "input source" or "untransformed source" vs. the "transformed source" or "output source" to refer to the subject's input and output.

If you wonder why I belabour this, it's because I personally find it confusing.

## Process

The test process uses Ninja internally to manage the execution of the test subjects, so the overall process proceeds in roughly this order:
1. Parse arguments and scan external configuration (right now mainly looking for `.qc` and `.qasm` files in `bench/resources`)
2. Create a timestamped build folder
3. Generate a Ninja build file with targets for the transformed source, run log, resource usage, and analysis of the transformed source
   - Generally the transformed source, run log, and resource usage record are generated by a single wrapper around the actual subject being run
   - The analysis of the transformed source is done by a separate build rule shared by all subjects
   - The TestSubject is responsible for emitting all these build targets, and returns a list of associated files
   - There's a phony target collecting all the test runs together
4. Run Ninja to actually generate test results
5. Report errors if any of the tests failed!
6. Scan the resource and analysis records, collect them into a single table by test, and write that to the build folder as `<suite>_<timestamp>_raw.csv`
7. Generate a summary table for the specified measurables, and write that to the build folder as `<suite>_<timestamp>_processed.csv`


## The Ninja Build

The data collection process is split into two steps:
1. Run the test subject optimizer and collect time and resource use stats
2. Analyze the optimized circuit or program to collect quality stats

The outputs for these steps are a folder of files in `bench/build/<suite>/<timestamp>`. The layout is roughly:
| Sub-path | Description |
|--|--|
| `<subject>/<resource>_opt[_<runid>].{qc,qasm}` | Transformed source output |
| `<subject>/<resource>_run[_<runid>].log` | Log file captured during run |
| `<resource>_usage[_<runid>].json` | Resource usage recorded during run |
| `<resource>_analysis[_<runid>].json` | Analysis of transformed source output |

In general the job of the TestSubject is to generate build targets for the first 3/4 things: the transformed source, the log, the resource usage record. The analysis targets can be generated by a separate build using a shared rule.

A couple notes on resource use and capturing runtimes:
- If we're measuring time, or we're worried about resource usage (i.e. we are nearing the physical limit of machine RAM), the Ninja file should use pools to prevent parallel execution of rules
- When measuring time remember other best practices as well, like:
   - Warm the caches by running the test once and throwing those results away (or flush it in a _repeatable_ way)
   - Run tests multiple times -- 10 is a good minimum in my experience -- to get a ballpark for the variance

