# Design

For information about updating the subtrees, see [subtrees](subtrees.md).

For information about the software being benchmarked (the test subjects), see the various .md files named after the included subtrees: [Feynman](feynman.md), [VOQC](mlvoqc.md), [Quartz](quartz.md), [QUESO](queso.md), [QuiZX](quizx.md), [TOpt](topt.md).

## Glossary

| Term | Meaning |
|--|--|
| Benchmark, benchmark suite, suite | A collection of tests, test subjects, and specific test procedures (including which data is collected from the tests) |
| Measurable | A data point of interest which is generated by a test, e.g. T-gate count, runtime, peak memory use |
| Subject, test subject | A program or framework being benchmarked, e.g. Feynman, Quartz, VOQC... |
| Resource | Some input to a test, for now just a `.qc` or `.qasm` file; note that the same resource might refer to both a `.qc` and an equivalent `.qasm`, to enable comparison of software that processes one format with software that processes the other |
| Test, benchmark test | A single test run as part of a benchmark suite, generally a particular quantum circuit or program being transformed by a particular subject with particular resource constraints and repeat count |
| Transformed source | The output from a test process -- for now the objective is _optimization_, but we somewhat arbitrarily avoid that term; the goal isn't always achieved, and maybe in future some other kind of transformation could be desired |

Note that "source" usually means `.qc` or `.qasm` source, i.e., the input _and output_ of the test subject. The nomenclature could then be "source" or "input source" or "untransformed source" vs. the "transformed source" or "output source" to refer to the subject's input and output.

If you wonder why I belabour this, it's because I personally find it confusing.

## Process

The test process uses Ninja internally to manage the execution of the test subjects, so the overall process proceeds in roughly this order:
1. Parse arguments and scan external configuration (right now mainly looking for `.qc` and `.qasm` files in `bench/resources`)
2. Create a timestamped build folder
3. Generate a Ninja build file with targets for the transformed source, run log, resource usage, and analysis of the transformed source
   - Generally the transformed source, run log, and resource usage record are generated by a single wrapper around the actual subject being run
   - The analysis of the transformed source is done by a separate build rule shared by all subjects
   - The TestSubject is responsible for emitting all these build targets, and returns a list of associated files
   - There's a phony target collecting all the test runs together
4. Run Ninja to actually generate test results
5. Report errors if any of the tests failed!
6. Scan the resource and analysis records, collect them into a single table by test, and write that to the build folder as `<suite>_<timestamp>_raw.csv`
7. Generate a summary table for the specified measurables, and write that to the build folder as `<suite>_<timestamp>_processed.csv`


## The Ninja Build

The data collection process is split into two steps:
1. Run the test subject optimizer and collect time and resource use stats
2. Analyze the optimized circuit or program to collect quality stats

The outputs for these steps are a folder of files in `bench/build/<suite>/<timestamp>`. The layout is roughly:
| Sub-path | Description |
|--|--|
| `<subject>/<resource>_opt[_<runid>].{qc,qasm}` | Transformed source output |
| `<subject>/<resource>_run[_<runid>].log` | Log file captured during run |
| `<resource>_usage[_<runid>].json` | Resource usage recorded during run |
| `<resource>_analysis[_<runid>].json` | Analysis of transformed source output |

In general the job of the TestSubject is to generate build targets for the first 3/4 things: the transformed source, the log, the resource usage record. The analysis targets can be generated by a separate build using a shared rule.

A couple notes on resource use and capturing runtimes:
- If we're measuring time, or we're worried about resource usage (i.e. we are nearing the physical limit of machine RAM), the Ninja file should use pools to prevent parallel execution of rules
- When measuring time remember other best practices as well, like:
   - Warm the caches by running the test once and throwing those results away (or flush it in a _repeatable_ way)
   - Run tests multiple times -- 10 is a good minimum in my experience -- to get a ballpark for the variance

